{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee008eeb",
   "metadata": {},
   "source": [
    "# **Tutorial on training the MNN (vanilla ver)**\n",
    "\n",
    "Here, we show a minimal example of training the MNN using the standard pytorch style without the wrapper. This is suited to those who require under-the-hood modifications of the model.\n",
    "\n",
    "If you don't already have **PyTorch** installed, you need to install it following the instruction on this page: https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "You need to copy this notebook to the root direction (under `moment-neural-network`).\n",
    "\n",
    "First, the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnn.mnn_core.mnn_pytorch import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73621a58",
   "metadata": {},
   "source": [
    "A quick check of your pytorch version and GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5402db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.7.0+cu126\n",
      "Using GPU, device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac66cc",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "PyTorch has two classes from [`torch.utils.data`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) to work with data: \n",
    "- [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) which represents the actual data items, such as images or pieces of text, and their labels\n",
    "- [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) which is used for processing the dataset in batches during training.\n",
    "\n",
    "Here we will use TorchVision and `torchvision.datasets` to access the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). (By setting `download=True`, the code below will attempt to download the dataset if it doesn't already exist locally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e552c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./datasets/', train=True, download=True,\n",
    "                transform=transforms.Compose([ToTensor(),\n",
    "                transforms.Normalize((0,), (1,))]))\n",
    "test_dataset = datasets.MNIST('./datasets/', train=False, download=True,\n",
    "                  transform=transforms.Compose([ToTensor(),\n",
    "                  transforms.Normalize((0,), (1,))]))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ca11c",
   "metadata": {},
   "source": [
    "The data loaders provide a way of iterating through the datasets in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30347c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "target: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# load the first batch of data\n",
    "for (data, target) in train_loader:\n",
    "    print('data:', data.size(), 'type:', data.type())\n",
    "    print('target:', target.size(), 'type:', target.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b17c2c",
   "metadata": {},
   "source": [
    "## Building a feedforward MNN\n",
    "\n",
    "A single feedforward layer of MNN consists of the following components:\n",
    "\n",
    "- **linear (bilinear) layer**: outputs synaptic current mean/covariance given pre-synaptic neuron spike mean/covariance. Accessed through the `LinearDuo` class under `mnn.mnn_core.nn.linear`.\n",
    "- **moment batch normalization**: outputs batch-normalized synaptic current mean/covariance. This is a generalization of standard batchnorm to second-order moments and is required to avoid vanishing gradient problem. Accessed through the `BatchNorm1dDuo` class under `mnn.mnn_core.nn.batchnorm`.\n",
    "- **moment activation**: outputs post-synaptic neuron spike mean/covairance, given input current mean/covariance. Accessed through the `OriginMnnActivation` class under `mnn.mnn_core.nn.activation`.\n",
    "\n",
    "They can be accessed individually or as a single block using the `EnsembleLinearDuo` class under `mnn.mnn_core.nn.ensemble`.\n",
    "\n",
    "A single feedforward layer can be stack multiple times to form a deep MNN. For illustrative purposes, here we show an example consisting of a single hidden layer with linear readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dccf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoNet(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers = 10, hidden_layer_size = 64, input_size = 2, output_size = 1):\n",
    "        super(MoNet, self).__init__()\n",
    "        self.layer_sizes = [input_size]+[hidden_layer_size]*num_hidden_layers+[output_size]  # input, hidden, output\n",
    "        \n",
    "        \n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [MomentLayer(self.layer_sizes[i], self.layer_sizes[i + 1]) for i in range(len(self.layer_sizes) - 1)])\n",
    "        \n",
    "        return\n",
    "\n",
    "    def forward(self, u, s, rho):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            u, s, rho = self.layers[i].forward(u, s, rho)\n",
    "        return u, s, rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e75bf1",
   "metadata": {},
   "source": [
    "## Training the MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fed7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000       \n",
    "batch_size = 32\n",
    "num_batches = int(sample_size/batch_size)\n",
    "num_epoch = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "\n",
    "model = MoNet(num_hidden_layers = config['num_hidden_layers'], hidden_layer_size = config['hidden_layer_size'], input_size = input_size, output_size = output_size)\n",
    "    \n",
    "train_dataset = Dataset(config['dataset_name'], sample_size = sample_size, input_dim = input_size, output_dim = output_size, with_corr = config['with_corr'], fixed_rho = config['fixed_rho'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size)        \n",
    "\n",
    "model.target_transform = train_dataset.transform\n",
    "\n",
    "validation_dataset = Dataset(config['dataset_name'], sample_size = 32, input_dim = input_size, output_dim = output_size, transform = train_dataset.transform, with_corr = config['with_corr'], fixed_rho = config['fixed_rho'] )          \n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size = 32)\n",
    "\n",
    "params = model.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr = lr, amsgrad = True) #recommended lr: 0.1 (Adam requires a much smaller learning rate than SGD otherwise won't converge)\n",
    "    \n",
    "for epoch in range(num_epoch):            \n",
    "    model.train()\n",
    "    for i_batch, sample in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()                \n",
    "        u, s, rho = model.forward(sample['input_data'][0], sample['input_data'][1], sample['input_data'][2])\n",
    "        loss = loss_mse_covariance(u, s, rho, sample['target_data'][0], sample['target_data'][1], sample['target_data'][2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('Training epoch {}/{}'.format(epoch,num_epoch))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i_batch, sample in enumerate(validation_dataloader):\n",
    "            u, s, rho = model.forward(sample['input_data'][0], sample['input_data'][1], sample['input_data'][2])\n",
    "            \n",
    "            loss = loss_function_mse(u, s, sample['target_data'][0], sample['target_data'][1])\n",
    "            \n",
    "            print('Validation loss:', loss.item())\n",
    "            print('Epoch:',epoch)\n",
    "\n",
    "#model.checkpoint['model_state_dict'] =  model.state_dict()\n",
    "#model.checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sophie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
