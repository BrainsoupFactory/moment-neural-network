{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee008eeb",
   "metadata": {},
   "source": [
    "# **Tutorial on training the MNN (vanilla ver)**\n",
    "\n",
    "Here, we show a minimal example of training the MNN using the standard pytorch style without the wrapper. This is suited to those who require under-the-hood modifications of the model.\n",
    "\n",
    "If you don't already have **PyTorch** installed, you need to install it following the instruction on this page: https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "You need to copy this notebook to the root directory (under `moment-neural-network`).\n",
    "\n",
    "First, the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988e61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnn.mnn_core.mnn_pytorch import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73621a58",
   "metadata": {},
   "source": [
    "A quick check of your pytorch version and GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5402db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.7.0+cu126\n",
      "Using GPU, device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac66cc",
   "metadata": {},
   "source": [
    "## Loading the data & input encoding\n",
    "\n",
    "PyTorch has two classes from [`torch.utils.data`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) to work with data: \n",
    "- [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) which represents the actual data items, such as images or pieces of text, and their labels\n",
    "- [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) which is used for processing the dataset in batches during training.\n",
    "\n",
    "Here we will use TorchVision and `torchvision.datasets` to access the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). (By setting `download=True`, the code below will attempt to download the dataset if it doesn't already exist locally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e552c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./datasets/', train=True, download=True,\n",
    "                transform=transforms.Compose([ToTensor(),\n",
    "                transforms.Normalize((0,), (1,))]))\n",
    "test_dataset = datasets.MNIST('./datasets/', train=False, download=True,\n",
    "                  transform=transforms.Compose([ToTensor(),\n",
    "                  transforms.Normalize((0,), (1,))]))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ca11c",
   "metadata": {},
   "source": [
    "The data loaders provide a way of iterating through the datasets in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30347c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "target: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# load the first batch of data\n",
    "for (data, target) in train_loader:\n",
    "    print('data:', data.size(), 'type:', data.type())\n",
    "    print('target:', target.size(), 'type:', target.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb2a15",
   "metadata": {},
   "source": [
    "We need to specify an appropriate encoding scheme of the input data to pass it to the MNN. Here, we suppose that the inputs are the statistical moments of independent Poisson spike trains whose firing rates are proportional to the image pixel values. Below is a helper function that implements this input encoding. The `scale` parameter describes how input pixel values should be converted to firing rates in sp/ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb75e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_mean: torch.Size([32, 784]) type: torch.FloatTensor\n",
      "input_cov: torch.Size([32, 784, 784]) type: torch.FloatTensor\n",
      "target: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "def input_encoder(data, scale=1):\n",
    "    data = torch.flatten(data, start_dim=1)\n",
    "    input_mean = data*scale\n",
    "    input_cov = torch.diag_embed(input_mean)\n",
    "    return input_mean, input_cov\n",
    "\n",
    "# load the first batch of data\n",
    "for (data, target) in train_loader:\n",
    "    input_mean, input_cov = input_encoder(data)\n",
    "    print('input_mean:', input_mean.size(), 'type:', data.type())\n",
    "    print('input_cov:', input_cov.size(), 'type:', data.type())\n",
    "    print('target:', target.size(), 'type:', target.type())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b17c2c",
   "metadata": {},
   "source": [
    "## Building a feedforward MNN\n",
    "\n",
    "A single feedforward layer of MNN consists of the following components:\n",
    "\n",
    "- **linear (bilinear) layer**: outputs synaptic current mean/covariance given pre-synaptic neuron spike mean/covariance. Accessed through the `LinearDuo` class under `mnn.mnn_core.nn.linear`.\n",
    "- **moment batch normalization**: outputs batch-normalized synaptic current mean/covariance. This is a generalization of standard batchnorm to second-order moments and is required to avoid vanishing gradient problem. Accessed through the `CustomBatchNorm1D` class under `mnn.mnn_core.nn.custom_batch_norm`.\n",
    "- **moment activation**: outputs post-synaptic neuron spike mean/covairance, given input current mean/covariance. Accessed through the `OriginMnnActivation` class under `mnn.mnn_core.nn.activation`.\n",
    "\n",
    "A single feedforward layer can be stack multiple times to form a deep MNN. For illustrative purposes, here we show an example consisting of a single hidden layer followed by a linear readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48dccf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnn.mnn_core.nn.activation import OriginMnnActivation\n",
    "from mnn.mnn_core.nn.linear import LinearDuo\n",
    "from mnn.mnn_core.nn.custom_batch_norm import CustomBatchNorm1D\n",
    "\n",
    "class SimpleMNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size = 64, input_size = 2, output_size = 1):\n",
    "        super(SimpleMNN, self).__init__()\n",
    "        self.linear = LinearDuo(input_size, hidden_size)\n",
    "        self.batchnorm = CustomBatchNorm1D(hidden_size)\n",
    "        self.activate = OriginMnnActivation()\n",
    "        self.readout = LinearDuo(hidden_size,output_size)        \n",
    "        return\n",
    "\n",
    "    def forward(self, input_mean, input_cov):\n",
    "        curr_mean, curr_cov = self.linear(input_mean, input_cov)\n",
    "        bn_mean, bn_cov = self.batchnorm(curr_mean, curr_cov)\n",
    "        hidden_mean, hidden_cov = self.activate(bn_mean, bn_cov)\n",
    "        readout_mean, readout_cov = self.readout(hidden_mean, hidden_cov)\n",
    "        return readout_mean, readout_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575e7b8",
   "metadata": {},
   "source": [
    "The following script creates an instance of the model and prints the name and shape of their trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab2e1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer:  LinearDuo(in_features: 784, out_features: 100, bias_mean: False, bias_var: False, dropout: False, scale: None)\n",
      "    Name: weight, Shape: torch.Size([100, 784])\n",
      "Moment batchnorm:  CustomBatchNorm1D(num_features: 100, bias_std=False, special_init=True, momentum=0.9, eps=1e-05, affine=True)\n",
      "    Name: weight, Shape: torch.Size([100])\n",
      "    Name: bias, Shape: torch.Size([100])\n",
      "Moment activation OriginMnnActivation()\n",
      "Readout layer: LinearDuo(in_features: 100, out_features: 10, bias_mean: False, bias_var: False, dropout: False, scale: None)\n",
      "    Name: weight, Shape: torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMNN(hidden_size=100, input_size=28*28,output_size=10)\n",
    "\n",
    "# you can inspect the trainable parameters\n",
    "print('Linear layer: ', model.linear)\n",
    "for name, param in model.linear.named_parameters():\n",
    "    print('    Name: {}, Shape: {}'.format(name, param.shape))\n",
    "print('Moment batchnorm: ', model.batchnorm)\n",
    "for name, param in model.batchnorm.named_parameters():\n",
    "    print('    Name: {}, Shape: {}'.format(name, param.shape))\n",
    "print('Moment activation', model.activate)\n",
    "print('Readout layer:', model.readout)\n",
    "for name, param in model.readout.named_parameters():\n",
    "    print('    Name: {}, Shape: {}'.format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e75bf1",
   "metadata": {},
   "source": [
    "## Training the MNN\n",
    "So far we have defined the dataset, the data loader, and the model. To train the model, we need to specify the loss function and the optimizer.\n",
    "\n",
    "For classification problems, we provide `CrossEntropyOnMean` and `GaussianSamplingCrossEntropyLoss` under `mnn.mnn_core.nn.criterion`. The former is identical to the standard cross-entropy loss in PyTorch, whereas the latter is a generalized cross-entropy taking into account of the second-order moments of the output. \n",
    "\n",
    "Below is a minimal example using the standard cross-entropy and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fed7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1...\n",
      "Validation accuracy = 94.43%\n"
     ]
    }
   ],
   "source": [
    "from mnn.mnn_core.nn.criterion import CrossEntropyOnMean\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch = 1\n",
    "lr = 0.01\n",
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "model = SimpleMNN(hidden_size = hidden_size, input_size = input_size, output_size = output_size)    \n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr = lr, amsgrad = True)\n",
    "criterion = CrossEntropyOnMean()\n",
    "\n",
    "for epoch in range(num_epoch):            \n",
    "    model.train()\n",
    "\n",
    "    print('Training epoch {}/{}...'.format(epoch,num_epoch))\n",
    "    for i_batch, (images, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_mean, input_cov = input_encoder(images) # encode input data to moment representation\n",
    "        output_mean, output_cov = model.forward(input_mean,input_cov) # run the forward pass\n",
    "        loss = criterion((output_mean, output_cov), target) # calculate the loss function\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update model parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        for i_batch, (images, target) in enumerate(test_loader):\n",
    "            input_mean, input_cov = input_encoder(images) \n",
    "            output_mean, output_cov = model.forward(input_mean,input_cov) \n",
    "            prediction = output_mean.argmax(1)  # index of the largest entry in the output mean\n",
    "            num_correct += torch.sum(prediction == target).item()  # count correct predictions\n",
    "        \n",
    "        acc = np.round(num_correct/len(test_dataset)*100,2)\n",
    "        print('Validation accuracy = {}%'.format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea3c57",
   "metadata": {},
   "source": [
    "We can access all the trained parameters of the model and the state of the optimizer using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66f7b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state dictionary:  odict_keys(['linear.weight', 'batchnorm.weight', 'batchnorm.bias', 'batchnorm.running_mean', 'batchnorm.running_var', 'readout.weight'])\n",
      "Optimizer state dictionary:  dict_keys(['state', 'param_groups'])\n"
     ]
    }
   ],
   "source": [
    "print('Model state dictionary: ', model.state_dict().keys())\n",
    "print('Optimizer state dictionary: ', optimizer.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a91fc",
   "metadata": {},
   "source": [
    "## Reconstruct spiking neural network\n",
    "\n",
    "As the MNN is derived from its corresponding spiking neural network (SNN) model (of current-based leaky integrate-and-fire neurons) on a mathematically rigorous ground, the trained parameters can be used to reconstruct the SNN without futher tuning. \n",
    "\n",
    "Note that the moment batchnorm is only required for training purposes, and we can simply absorb its parameters into the linear layer, using the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e029947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def weight_fusion(ln, bn):\n",
    "    ln_weight = ln.weight.detach()\n",
    "    bn_weight = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "    bn_weight = bn_weight.detach()\n",
    "    weight = ln_weight * bn_weight.unsqueeze(-1)\n",
    "    bias = -bn.running_mean * bn_weight + bn.bias\n",
    "    return weight, bias\n",
    "\n",
    "weight, bias = weight_fusion(model.linear,model.batchnorm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e6d2a",
   "metadata": {},
   "source": [
    "These `weight` and `bias` can then be used to reconstruct the SNN that will generate the same firing statistics as in the MNN.\n",
    "\n",
    "## Exercises\n",
    "1. Try modifying `SimpleMNN` by stacking multiple hidden layers to form a deep MNN.\n",
    "2. Replace the task with a regression problem and also the loss function accordingly. Hint: see `MSEOnMean` and `LikelihoodMSE` under `mnn.mnn_core.nn.criterion`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sophie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
