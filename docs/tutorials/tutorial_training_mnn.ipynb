{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e95807f",
   "metadata": {},
   "source": [
    "# Tutorial on MNN training\n",
    "## Quick start: three steps to run your first MNN model\n",
    "\n",
    "The following provides a step-by-step instruction to train an MNN to learn MNIST image classification task with a multi-layer perceptron structure.\n",
    "\n",
    "1. Clone the repository to your local drive.\n",
    "2. Copy the demo files, **./example/mnist/mnist.py** and **./example/mnist/mnist_config.yaml** to the root directory.\n",
    "3. Create two directories, **./checkpoint/** (for saving trained model results) and **./data/** (for downloading the MNIST dataset).\n",
    "4. Run the following command to call the script named `mnist.py` with the config file specified through the option:\n",
    "\n",
    "   ```\n",
    "   python mnist.py --config=./mnist_config.yaml\n",
    "   ```\n",
    "\n",
    "After training is finished, you should find four files in the **./checkpoint/mnist/** folderï¼š\n",
    "\n",
    "- Two '.pth' files which contain the trained model parameters.\n",
    "- One '.yaml' file which is a copy of the config file used for running the training the model.\n",
    "- One '.txt' log file that prints the standard output during training (such as model performance).\n",
    "- One directroy called `mnn_net_snn_result` that stores the simulation result of the SNN reconstructed from the trained MNN (if enabled)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f8d4f",
   "metadata": {},
   "source": [
    "## A step-by-step explaination of how the MNN is trained\n",
    "\n",
    "Here we will illustate how the above codes work.\n",
    "Before we start, we need load the reqired package.\n",
    "Since MNN is still at a early stage such that we do not yet publish it on Pypi so you need to copy this notebook to root directory of the repo (moment-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9cdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mnn import snn, models, utils\n",
    "from mnn.utils.training_tools import general_train, general_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c4654",
   "metadata": {},
   "source": [
    "When calling the script\n",
    "   ```\n",
    "   python mnist.py --config=./mnist_config.yaml\n",
    "   ```\n",
    "It will first load all necessary hyperparameters for training, which is equivalent to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2462c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs: 50\n",
      "print_freq: 20\n",
      "dir: mnist\n",
      "save_name: mnn_mnist\n",
      "use_cuda: True\n",
      "seed: None\n",
      "resume: False\n",
      "distributed: False\n",
      "evaluate: False\n",
      "start_epoch: 0\n",
      "local_rank: 0\n",
      "config: ./examples/mnist/mnist_config.yaml\n",
      "LR_SCHEDULER: None\n",
      "OPTIMIZER: {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 0.01}}\n",
      "DATASET: None\n",
      "DATALOADER: None\n",
      "MODEL: {'meta': {'arch': 'mnn_mlp', 'cnn_type': None, 'mlp_type': 'mnn_mlp'}, 'mnn_mlp': {'structure': [784, 100], 'num_class': 10, 'bn_bias_var': False, 'predict_bias': True, 'predict_bias_var': False, 'special_init': True, 'dropout': None, 'momentum': 0.9, 'eps': 1e-05}, 'snn_mlp': {'structure': [784, 800], 'num_class': 10, 'use_cov': False, 'bn_bias_var': False}}\n",
      "CRITERION: {'name': 'CrossEntropyOnMean', 'source': 'mnn_core', 'args': {'reduction': 'mean'}}\n",
      "DATAAUG_TRAIN: {'aug_order': ['ToTensor'], 'RandomCrop': {'size': 28, 'padding': 2}}\n",
      "DATAAUG_VAL: {'aug_order': ['ToTensor']}\n",
      "workers: 2\n",
      "lr: 0.001\n",
      "epochs: 1\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "dataset: mnist\n",
      "dataset_type: classic\n",
      "input_prepare: flatten_poisson\n",
      "save_epoch_state: False\n",
      "scale_factor: 1.0\n",
      "data_dir: ./data/\n",
      "task_type: classification\n",
      "background_noise: None\n",
      "dump_path: ./checkpoint/\n"
     ]
    }
   ],
   "source": [
    "class TempArgs:\n",
    "    def __init__(self):\n",
    "        self.bs = 50 # batch size\n",
    "        self.print_freq = 20 # print frequency\n",
    "        self.dir = 'mnist' # directory name to save the model\n",
    "        self.save_name = 'mnn_mnist' # name of the model to save\n",
    "        self.use_cuda = True # whether to use cuda\n",
    "        self.seed = None # random seed\n",
    "        self.resume = False # whether to resume training from a checkpoint\n",
    "        self.distributed = False # whether to use distributed training\n",
    "        self.evaluate = False # whether to evaluate the model only\n",
    "        self.start_epoch = 0 # starting epoch\n",
    "        self.local_rank = 0 # local rank for distributed training\n",
    "\n",
    "args = TempArgs()\n",
    "setattr(args, 'config', './examples/mnist/mnist_config.yaml') # path to the config file\n",
    "args = general_prepare.set_config2args(args)\n",
    "for key, value in args.__dict__.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028471",
   "metadata": {},
   "source": [
    "In this configuration,\n",
    "* `MODEL` specifyy the network architecture and hyperparameters necessciate to constructe a MNN model. In this tutorial, the configuration will create a MNN model that using MLP architecture consist of one hidden layer (100 neurons)\n",
    "* `OPTIMIZER` specify which optimizer (offered by Pytorch) should we used and its hyperparameters\n",
    "* `CRITERION` specify the criterion used to compute the output loss for optimizing the model. `source` further specify where to get the corresponding module. By default we will use `CrossEntropyOnMean`\n",
    "* `DATASET` and `DATALOADER` are reserved args for those who want customize their own dataset. By default, we use the dataset provided by torchvision, where `dataset` specify which dataset is used and `data_dir` is the path where dataset is stored. In this tutorial, we use MNIST. The batch size is specified by `bs` and the training epochs is specified by `epochs`\n",
    "* `DATAAUG_TRAIN` and `DATAAUG_VAL` specify the way to do data augmentation.\n",
    "  \n",
    "After setting up all necessary hyperparameters, we can run the following code to start training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fcd1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/1200]\tTime  0.024 ( 0.024)\tData  0.007 ( 0.007)\tLoss 2.3007e+00 (2.3007e+00)\tAcc@1   8.00 (  8.00)\n",
      "Epoch: [0][ 100/1200]\tTime  0.030 ( 0.028)\tData  0.004 ( 0.004)\tLoss 1.9582e+00 (2.0891e+00)\tAcc@1  70.00 ( 64.28)\n",
      "Epoch: [0][ 200/1200]\tTime  0.031 ( 0.029)\tData  0.004 ( 0.004)\tLoss 1.6539e+00 (1.9580e+00)\tAcc@1  88.00 ( 71.00)\n",
      "Epoch: [0][ 300/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 1.4744e+00 (1.8345e+00)\tAcc@1  82.00 ( 74.86)\n",
      "Epoch: [0][ 400/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 1.2726e+00 (1.7258e+00)\tAcc@1  90.00 ( 76.98)\n",
      "Epoch: [0][ 500/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 1.2473e+00 (1.6237e+00)\tAcc@1  78.00 ( 78.71)\n",
      "Epoch: [0][ 600/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 1.0248e+00 (1.5336e+00)\tAcc@1  90.00 ( 79.94)\n",
      "Epoch: [0][ 700/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 9.2382e-01 (1.4497e+00)\tAcc@1  92.00 ( 81.05)\n",
      "Epoch: [0][ 800/1200]\tTime  0.030 ( 0.030)\tData  0.004 ( 0.004)\tLoss 7.5740e-01 (1.3746e+00)\tAcc@1  90.00 ( 81.96)\n",
      "Epoch: [0][ 900/1200]\tTime  0.029 ( 0.030)\tData  0.004 ( 0.004)\tLoss 8.4661e-01 (1.3089e+00)\tAcc@1  76.00 ( 82.67)\n",
      "Epoch: [0][1000/1200]\tTime  0.029 ( 0.030)\tData  0.004 ( 0.004)\tLoss 7.5224e-01 (1.2495e+00)\tAcc@1  84.00 ( 83.27)\n",
      "Epoch: [0][1100/1200]\tTime  0.029 ( 0.030)\tData  0.004 ( 0.004)\tLoss 6.3211e-01 (1.1956e+00)\tAcc@1  88.00 ( 83.74)\n",
      "Epoch: [0] * Time 0.030 Data 0.004 Loss 1.149 Acc@1 84.182\n",
      "Test [Epoch:0]:  * Time 0.014 Data 0.000 Loss 0.574 Acc@1 90.040\n"
     ]
    }
   ],
   "source": [
    "args.print_freq = 100 # set print frequency to 100 for this example\n",
    "# Simlarly, you can set other hyperparameters as needed\n",
    "general_train.general_train_pipeline(args, train_func=general_train.TrainProcessCollections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b6b5a",
   "metadata": {},
   "source": [
    "When training is finished, you are will find a directory named by `dir` in the path specified by `dump_path`. The directory will contain four files named by `save_name` with different suffix:\n",
    "* `*_config.yaml` recorded all hyperparamters used in training so you can reproduce the experiments.\n",
    "* `*_log.txt` recorded the loss and accuracy of the model during the training process\n",
    "* `*.pth` contained the model parameters at the last epoch.\n",
    "* `*_best_model.pth` contained the model parameters that hit the highest accuracy on the validation set during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dab31e",
   "metadata": {},
   "source": [
    "## Reconstruct SNN based on trained MNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ba9be",
   "metadata": {},
   "source": [
    "The parameters of MNN can be directly used in SNN without further fine tuning.\n",
    "We also provided a pipeline to recontructe SNN based on trained MNN and run simulation by using the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9680397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set, Img idx: 0, target: 7, pred: tensor([7])\n",
      "test set, Img idx: 1, target: 2, pred: tensor([2])\n",
      "test set, Img idx: 2, target: 1, pred: tensor([1])\n",
      "test set, Img idx: 3, target: 0, pred: tensor([0])\n",
      "test set, Img idx: 4, target: 4, pred: tensor([4])\n"
     ]
    }
   ],
   "source": [
    "dt = 1 # time step for simulation\n",
    "input_type = 'poisson' # Using Poisson process to generate input spikes\n",
    "num_trial = 100 # number of trials for validation\n",
    "running_time = 100 # running time for each trial in ms\n",
    "pregenerate = False # whether to pregenerate the input spikes\n",
    "m = snn.functional.MnnSnnValidate(args, running_time=running_time, dt=dt, num_trials=num_trial, \n",
    "pregenerate=pregenerate, resume_best=False, input_type=input_type)\n",
    "for index in range(5): # run simuations with the first 5 samples in the validation set\n",
    "    m.validate_one_sample(index, do_reset=True, dump_spike_train=True, record=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401acf3f",
   "metadata": {},
   "source": [
    "You will find another directory in the `dir` that named by `save_name` with a suffix `_snn_validate_result`.\n",
    "There a two type file:\n",
    "* `*.snnval` stored the information of running the simulation such as spike count and simulation duration.\n",
    "* `*.spt` stored the spike trains of hidden neurons during the simulation, which is stored as sparse tensor with the shape `(int(running_time/dt), num_trial, hidden_neurons)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elephant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
